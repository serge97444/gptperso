{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "tavily_api_key =\"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\"\n",
    "\n",
    "from llama_index.tools.tavily_research.base import TavilyToolSpec\n",
    "\n",
    "tavily_tool = TavilyToolSpec(tavily_api_key )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.tools.duckduckgo import DuckDuckGoSearchToolSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CTransformersModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "The model `./LLMs/Mistralv1.02` and tokenizer `./LLMs/Mistralv1.02/tokenizer` are different, please ensure that they are compatible.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer , AutoModelForCausalLM , pipeline\n",
    "import ctransformers as CT\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "\n",
    "llm = CT.AutoModelForCausalLM.from_pretrained('./LLMs/Mistralv1.02',context_length = 5000 , hf = True)\n",
    "\n",
    "\n",
    "from llama_index.llms.vllm import Vllm\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('./LLMs/Mistralv1.02/tokenizer')\n",
    "\n",
    "def messages_to_prompt(messages):\n",
    "    prompt = \"\\n\".join([str(x) for x in messages])\n",
    "    return f\"<s>[INST] {prompt} [/INST] </s>\\n\"\n",
    "\n",
    "def completion_to_prompt(completion):\n",
    "    return f\"<s>[INST] {completion} [/INST] </s>\\n\"\n",
    "\n",
    "# llm = Vllm(model='./LLMs/Mistralv1.02/mistral-7b-instruct-v0.2.Q8_0.gguf' ,\n",
    "#             #tokenizer='./LLMs/Mistralv1.02/tokenizer',\n",
    "#             tensor_parallel_size=2,\n",
    "#             # speculative_config = {'model': './LLMs/llama_hermes'}\n",
    "#             messages_to_prompt=messages_to_prompt,\n",
    "#             completion_to_prompt=completion_to_prompt)\n",
    "#pipe = pipeline('text-generation', model=llm, tokenizer=tokenizer,max_new_tokens = 5000 ,device=\"cuda\")\n",
    "hf = HuggingFaceLLM(model=llm, tokenizer= tokenizer, context_window=5000,max_new_tokens = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tavily_tool.search(\"what is the weather in Paris?\", max_results=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.core.agent.workflow import AgentWorkflow,FunctionAgent,ReActAgent\n",
    "\n",
    "# Query_agent=ReActAgent( name=\"Queryier\",\n",
    "#                        tools = [tavily_tool.to_tool_list()[0]] ,\n",
    "#                     llm=hf, verbose =True,\n",
    "#                        description= \"an agent looking for info in the web , later on a database too\",\n",
    "#                        system_prompt=\"You are an agent design either to answer directly to the user or \\\n",
    "#                        to search in the web the info asked by the user using your tools. Please use at least 5 as max_result parameter in Tavily to help you.\\\n",
    "#                         Please summarize the result in a clear final answer\")\n",
    "\n",
    "# result = Query_agent.run('Can you tell me if it is sunny in Paris? Why?')\n",
    "# # Summary_Agent= ReactAgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tavily_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "   from typing import Annotated\n",
    "    \n",
    "   from llama_index.core.tools import FunctionTool\n",
    "\n",
    "   def buy_actions(number_of_stocks : Annotated[str,\"A number of stocks to be bought o nthe market\"])->str:\n",
    "      \"\"\"A function allowing a portfolio manager to buy stocks on the market\n",
    "\n",
    "      Args:\n",
    "         number_of_stocks (str): the number of stocks to be bought\n",
    "\n",
    "      Returns:\n",
    "         str: the number of bought actions\n",
    "       \"\"\"\n",
    "      final_result = f\"we bought {number_of_stocks} stocks\"\n",
    "      return final_result\n",
    "    \n",
    "   buy_func = FunctionTool.from_defaults(buy_actions)\n",
    "        \n",
    "\n",
    "   llm = CT.AutoModelForCausalLM.from_pretrained('./LLMs/Mistralv1.02',context_length = 5000 , hf = True)\n",
    "\n",
    "\n",
    "   from llama_index.llms.vllm import Vllm\n",
    "\n",
    "   tokenizer = AutoTokenizer.from_pretrained('./LLMs/Mistralv1.02/tokenizer')\n",
    "\n",
    "\n",
    "    #pipe = pipeline('text-generation', model=llm, tokenizer=tokenizer,max_new_tokens = 5000 ,device=\"cuda\")\n",
    "   hf = HuggingFaceLLM(model=llm, tokenizer= tokenizer, context_window=5000,max_new_tokens = 512)\n",
    "\n",
    "   Query_agent=ReActAgent( name=\"Queryier\",\n",
    "                       tools = [tavily_tool.to_tool_list()[0] , buy_func] ,\n",
    "                    llm=hf, verbose =True,\n",
    "                       description= \"an agent looking for info in the web , later on a database too\",\n",
    "                       system_prompt=\"You are an agent design either to answer directly to the user or \\\n",
    "                       to search in the web the info asked by the user using your tools. Please use at least 5 as max_result parameter in Tavily to help you.\\\n",
    "                        Please summarize the result in a clear final answer\")\n",
    "\n",
    "   result = await Query_agent.run('How much is AXA Stok price now? If the AXA stock price is over 80 euros on the Paris stock market place , please buy 12 of them')\n",
    "\n",
    "   print(result)\n",
    "\n",
    "   return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
